# -*- coding: utf-8 -*-
"""Approach1-lemma_vader_tf-idf

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SQ-VKOcpe5oABT_LOiJz2eh1dNdf4Zuk

# Approach I  - Lemmatization + VADER + TF-IDF
"""

import pandas as pd
import re
from nltk.corpus import stopwords
import nltk
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import spacy
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

pip install vaderSentiment

# NLTK Downloads
nltk.download('stopwords')
nltk.download('wordnet')
# Load SpaCy's English language model
nlp = spacy.load("en_core_web_sm")

file_path = 'HateXplain-Dataset.xlsx'
df = pd.read_excel(file_path)

print(df.columns)
df.head()

print(f"Dataset size (total number of elements): {df.size}")
print(f"Dataset shape (rows, columns): {df.shape}")

# Filter for rows where target is 'ethnicity'
ethnicity_df = df[df['target'] == 'ethnicity']

# Display the shape and first few rows of the filtered dataset
print(f"Ethnicity dataset shape: {ethnicity_df.shape}")
ethnicity_df.head()

# Filter for rows where target is NaN
nan_target_df = df[df['target'].isnull()]

# Display the shape and first few rows of the filtered dataset
print(f"NaN target dataset shape: {nan_target_df.shape}")
nan_target_df.head()

# Combine the two datasets (ethnicity and NaN target)
final_data = pd.concat([ethnicity_df, nan_target_df])

# Reset the index for the new DataFrame
final_data = final_data.reset_index(drop=True)

# Verify the 'target' column remains intact
print("Unique targets in the final dataset:")
print(final_data['target'].unique())

# Verify the 'label' column remains intact
print("Unique labels in the final dataset:")
print(final_data['label'].unique())

# Display the shape of the final dataset
print(f"Final dataset shape: {final_data.shape}")
final_data.head()

"""**Data Preprocessing**



"""

#drop unwanted columns
final_data = final_data.drop(['Unnamed: 0', 'origin_id'], axis=1)
final_data.head()

# Check for missing values in the dataset
print("Missing values per column:")
print(final_data.isnull().sum())

"""Since there's no missing values, we r not handling the missing values"""

#Remove unnecessary characters, such as mentions (@user), URLs, and special characters,

def clean_text(text):
    text = re.sub(r'@\w+', '', text)  # Remove mentions
    text = re.sub(r'http\S+|www\S+', '', text)  # Remove URLs
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove non-alphanumeric characters
    text = text.lower()  # Convert to lowercase
    text = text.strip()  # Remove leading/trailing spaces
    return text

# Apply cleaning to the 'text' column
final_data['text'] = final_data['text'].apply(clean_text)

# Display a few cleaned rows
final_data.head()

#binary label encoding

# Map 'hs' (hate speech, in this context xenophobic) to 1 and 'nhs' (non-hate speech, here non-xenophobic) to 0
final_data['label'] = final_data['label'].map({'hs': 1, 'nhs': 0})

# Verify the transformation
print(final_data['label'].value_counts())

#Stopwords removal

# Define stop words set
stop_words = set(stopwords.words('english'))

# Add custom stop words
custom_stop_words = {"make", "anything", "user", "say", "right", "someone","number","go","something","know","want","everyone","even","your", "u","look","many","though","im","says","lol","oh","made","got","cause","around","well","put","give","trying","got","calling", "saying","still","us","must","much","lot"}
stop_words.update(custom_stop_words)

# Remove stop words from the 'text' column
final_data['text'] = final_data['text'].apply(
    lambda x: ' '.join([word for word in x.split() if word not in stop_words])
)

# Display a few rows to verify
final_data.head()

#WordCloud

# Combine all text into one string
all_text = ' '.join(final_data['text'])

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Plot the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud of Text Data", fontsize=16)
plt.show()

"""## Approach I -  Lemmatization + TF-IDF +VADER_Score"""

# Function for POS tagging and extracting key information
def preprocess_xenophobia_text(text):
    doc = nlp(text)
    # Extract lemmatized text
    lemmatized = ' '.join([token.lemma_ for token in doc])
    return {
        "lemmatized": lemmatized
    }

# Apply preprocessing to your dataset
final_data['preprocessed'] = final_data['text'].apply(preprocess_xenophobia_text)

# Extract components from the preprocessed column
final_data['lemmatized'] = final_data['preprocessed'].apply(lambda x: x['lemmatized'])
final_data.drop('preprocessed', axis=1, inplace=True)

# Display the processed dataset
final_data.head()

#WordCloud for lemmatized field

# Combine all text into one string
all_text = ' '.join(final_data['lemmatized'])

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Plot the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud of Lemmatized Data", fontsize=16)
plt.show()

#computer Vader score

# Initialize VADER Sentiment Analyzer
analyzer = SentimentIntensityAnalyzer()

# Function to calculate VADER scores
def compute_vader_scores(text):
    sentiment = analyzer.polarity_scores(text)
    return sentiment

# Apply VADER to the lemmatized text column
final_data['vader_scores'] = final_data['lemmatized'].apply(compute_vader_scores)

# Split VADER scores into separate columns
final_data['vader_neg'] = final_data['vader_scores'].apply(lambda x: x['neg'])
final_data['vader_neu'] = final_data['vader_scores'].apply(lambda x: x['neu'])
final_data['vader_pos'] = final_data['vader_scores'].apply(lambda x: x['pos'])
final_data['vader_compound'] = final_data['vader_scores'].apply(lambda x: x['compound'])

# Drop the combined VADER scores dictionary if not needed
final_data = final_data.drop(columns=['vader_scores'])

# Display updated DataFrame
final_data.head()

"""# Vectorize the Lemmatized Text"""

# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=5000)  # Use top 5000 features for efficiency

# Fit and transform the lemmatized text
lemmatized_tfidf = tfidf.fit_transform(final_data['lemmatized'])

# Convert TF-IDF matrix to a DataFrame
lemmatized_tfidf_df = pd.DataFrame(lemmatized_tfidf.toarray(), columns=tfidf.get_feature_names_out())

"""# Combine all the features"""

import pandas as pd
# Combine TF-IDF features, NER features, and VADER scores
combined_features = pd.concat([
    lemmatized_tfidf_df,  # TF-IDF features
    final_data[['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound']]  # VADER sentiment scores
], axis=1)

# Check final feature matrix shape
print(f"Final feature matrix shape: {combined_features.shape}")

"""# Split the dataset as test and training set"""

y = final_data['label']  # Target labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape}")
print(f"Testing set size: {X_test.shape}")

"""# Train the Model

# 1. Logistic regression
"""

from sklearn.linear_model import LogisticRegression

# Initialize and train logistic regression
model = LogisticRegression(max_iter=500)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

from sklearn.metrics import classification_report, accuracy_score

# Print evaluation metrics
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""# 2. Support Vector Machine (SVM)"""

from sklearn.svm import SVC

# Initialize SVM
svm = SVC(kernel='linear', probability=True)

# Train the model
svm.fit(X_train, y_train)

# Predict and evaluate
y_pred = svm.predict(X_test)
print("SVM Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

from sklearn.svm import SVC

# Initialize SVM
svm = SVC(kernel='rbf', probability=True)

# Train the model
svm.fit(X_train, y_train)

# Predict and evaluate
y_pred = svm.predict(X_test)
print("SVM Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""# 5. XGBoost"""

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score

# Convert Pandas DataFrames to NumPy arrays
X_train = X_train.values
X_test = X_test.values
# Initialize XGBoost
xgboost = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Train the model
xgboost.fit(X_train, y_train)

# Predict on test data
y_pred = xgboost.predict(X_test)

# Evaluate the model
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))